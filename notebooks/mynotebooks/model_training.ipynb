{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 1: Cài đặt các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install split-folders\n",
    "!pip install albumentations\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import albumentations as A\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 2: Đặt đường dẫn home chứa folder project hiện tại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    home_dir = '/content/drive/MyDrive/ClassificationofMangoDiseases'\n",
    "elif platform.system() == 'Windows':\n",
    "    home_dir = 'D:\\Projects\\ClassificationofMangoDiseases'\n",
    "else:\n",
    "    raise ValueError('Unsupported platform')\n",
    "\n",
    "print(f'Home directory: {home_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 3: Kết nối với Google Drive nếu đang chạy trên Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 4: Đặt đường dẫn tới tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn tới tập dữ liệu đã chọn (gốc hoặc đã remove background)\n",
    "data_dir = os.path.join(home_dir, 'data/MangoFruitDDS/SenMangoFruitDDS_bgremoved')  # Hoặc 'SenMangoFruitDDS_original'\n",
    "output_dir = os.path.join(home_dir, 'data/processed')\n",
    "augmented_dir = os.path.join(home_dir, 'data/augmented')\n",
    "\n",
    "print(f'Data directory: {data_dir}')\n",
    "print(f'Output directory: {output_dir}')\n",
    "print(f'Augmented directory: {augmented_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 5: Tiền xử lý dữ liệu và phân chia tập dữ liệu\n",
    "\n",
    "Sử dụng thư viện `split-folders` để phân chia tập dữ liệu thành các tập `train`, `valid`, và `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa thư mục đầu ra nếu đã tồn tại\n",
    "if os.path.exists(output_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "# Phân chia dữ liệu\n",
    "splitfolders.ratio(data_dir, output=output_dir, seed=42, ratio=(.7, .2, .1), group_prefix=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 6: Tăng cường dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa các phép tăng cường dữ liệu\n",
    "augmentations = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.Transpose(),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(),\n",
    "        A.MultiplicativeNoise()\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "    ], p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=0.2),\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(p=0.3),\n",
    "        A.GridDistortion(p=0.1),\n",
    "        A.IAAPiecewiseAffine(p=0.3),\n",
    "    ], p=0.2),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=2),\n",
    "        A.IAASharpen(),\n",
    "        A.IAAEmboss(),\n",
    "        A.RandomBrightnessContrast(),\n",
    "    ], p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "])\n",
    "\n",
    "# Tạo thư mục đầu ra cho dữ liệu tăng cường\n",
    "if not os.path.exists(augmented_dir):\n",
    "    os.makedirs(augmented_dir)\n",
    "\n",
    "# Áp dụng tăng cường dữ liệu\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(root, file)\n",
    "            img = np.array(Image.open(img_path))\n",
    "            augmented_img = augmentations(image=img)['image']\n",
    "            augmented_img = Image.fromarray(augmented_img)\n",
    "            output_path = img_path.replace(output_dir, augmented_dir)\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            augmented_img.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 7: Hiển thị thông tin thư mục sau khi phân chia và tăng cường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra các thư mục sau khi phân chia\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    level = root.replace(output_dir, '').count(os.sep)\n",
    "    indent = ' ' * 4 * (level)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    subindent = ' ' * 4 * (level + 1)\n",
    "    for f in files:\n",
    "        print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 8: Định nghĩa các hàm tạo mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define common hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define common loss function and optimizer\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define image size and input shape\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "# Function to create a simple CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to create a ResNet model\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to create an Inception model\n",
    "def create_inception_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.InceptionV3(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to create a DenseNet model\n",
    "def create_densenet_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.DenseNet121(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Function to create a model using Transfer Learning\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.VGG16(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bước 9: Tạo các DataLoader và huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa hàm huấn luyện và lưu mô hình\n",
    "def train_and_save_model(model, train_data, val_data, model_name, save_dir):\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_data\n",
    "    )\n",
    "    model.save(os.path.join(save_dir, model_name + '.h5'))\n",
    "    return history\n",
    "\n",
    "# Thiết lập đường dẫn lưu mô hình\n",
    "if platform.system() == 'Linux':\n",
    "    model_save_dir = '/content/drive/MyDrive/ClassificationofMangoDiseases/models'\n",
    "elif platform.system() == 'Windows':\n",
    "    model_save_dir = 'D:\\Projects\\ClassificationofMangoDiseases\\models'\n",
    "else:\n",
    "    raise ValueError('Unsupported platform')\n",
    "\n",
    "# Đảm bảo thư mục lưu trữ tồn tại\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Tạo data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    os.path.join(output_dir, 'train'),\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    os.path.join(output_dir, 'val'),\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "aug_train_data = train_datagen.flow_from_directory(\n",
    "    os.path.join(augmented_dir, 'train'),\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "aug_val_data = val_datagen.flow_from_directory(\n",
    "    os.path.join(augmented_dir, 'val'),\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "# Define the number of classes (adjust this according to your dataset)\n",
    "num_classes = len(train_data.class_indices)\n",
    "\n",
    "# Tạo và huấn luyện các mô hình\n",
    "model_names = ['CNNCustomOri', 'ResNet50Ori', 'InceptionV3Ori', 'DenseNet121Ori', 'VGG16Ori', \n",
    "               'CNNCustomAug', 'ResNet50Aug', 'InceptionV3Aug', 'DenseNet121Aug', 'VGG16Aug']\n",
    "\n",
    "# Original data models\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "resnet_model = create_resnet_model(input_shape, num_classes)\n",
    "inception_model = create_inception_model(input_shape, num_classes)\n",
    "densenet_model = create_densenet_model(input_shape, num_classes)\n",
    "vgg16_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "\n",
    "models_ori = [cnn_model, resnet_model, inception_model, densenet_model, vgg16_model]\n",
    "\n",
    "# Augmented data models\n",
    "cnn_model_aug = create_cnn_model(input_shape, num_classes)\n",
    "resnet_model_aug = create_resnet_model(input_shape, num_classes)\n",
    "inception_model_aug = create_inception_model(input_shape, num_classes)\n",
    "densenet_model_aug = create_densenet_model(input_shape, num_classes)\n",
    "vgg16_model_aug = create_transfer_learning_model(input_shape, num_classes)\n",
    "\n",
    "models_aug = [cnn_model_aug, resnet_model_aug, inception_model_aug, densenet_model_aug, vgg16_model_aug]\n",
    "\n",
    "# Compile the models\n",
    "for model in models_ori + models_aug:\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "# Train models on original data\n",
    "for model, name in zip(models_ori, model_names[:5]):\n",
    "    train_and_save_model(model, train_data, val_data, name, model_save_dir)\n",
    "\n",
    "# Train models on augmented data\n",
    "for model, name in zip(models_aug, model_names[5:]):\n",
    "    train_and_save_model(model, aug_train_data, aug_val_data, name, model_save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
