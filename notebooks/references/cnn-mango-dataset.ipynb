{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6450350,"sourceType":"datasetVersion","datasetId":3723789}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import image as mp_image\nimport seaborn as sns\n\n# Required magic to display matplotlib plots in notebooks\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport shutil\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T17:02:41.435736Z","iopub.execute_input":"2023-11-20T17:02:41.436168Z","iopub.status.idle":"2023-11-20T17:02:41.444153Z","shell.execute_reply.started":"2023-11-20T17:02:41.436132Z","shell.execute_reply":"2023-11-20T17:02:41.443106Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport shutil\n\n# Set the path to the SenMangoFruitDDS dataset\ntraining_folder_name = '/kaggle/input/mangofruitdds/MangoFruitDDS/SenMangoFruitDDS_original'\n# Define the classes based on the diseases and \"Healthy\" category\nclasses = ['Alternaria', 'Anthracnose', 'Black_Mould_Rot', 'Stem_and_Rot', 'Healthy']\n\n# Set image size for the CNN model\nimg_size = (128, 128)\n\n# Create a folder for resized images\ntrain_folder = '/kaggle/working/Mango'","metadata":{"execution":{"iopub.status.busy":"2023-11-20T17:03:19.638328Z","iopub.execute_input":"2023-11-20T17:03:19.638743Z","iopub.status.idle":"2023-11-20T17:03:19.646579Z","shell.execute_reply.started":"2023-11-20T17:03:19.638713Z","shell.execute_reply":"2023-11-20T17:03:19.645119Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Function to resize image\ndef resize_image(src_image, size=(128, 128), bg_color=\"white\"):\n    src_image.thumbnail(size, Image.LANCZOS)\n    new_image = Image.new(\"RGB\", size, bg_color)\n    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n    return new_image\n\n# Create resized copies of all the images\nsize = (128, 128)\n\nif os.path.exists(train_folder):\n    shutil.rmtree(train_folder)\n\nfor root, folders, files in os.walk(training_folder_name):\n    for sub_folder in folders:\n        save_folder = os.path.join(train_folder, sub_folder)\n        if not os.path.exists(save_folder):\n            os.makedirs(save_folder)\n        file_names = os.listdir(os.path.join(root, sub_folder))\n        for file_name in file_names:\n            file_path = os.path.join(root, sub_folder, file_name)\n            image = Image.open(file_path)\n            resized_image = resize_image(image, size)\n            save_as = os.path.join(save_folder, file_name)\n            resized_image.save(save_as)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.636860Z","iopub.execute_input":"2023-11-20T16:59:15.640436Z","iopub.status.idle":"2023-11-20T16:59:15.650118Z","shell.execute_reply.started":"2023-11-20T16:59:15.640388Z","shell.execute_reply":"2023-11-20T16:59:15.649222Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Data augmentation and preprocessing\ndata_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load the dataset\nfull_dataset = torchvision.datasets.ImageFolder(\n    root=train_folder,\n    transform=data_transform\n)\n\n# Split into training and testing datasets\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.652132Z","iopub.execute_input":"2023-11-20T16:59:15.652439Z","iopub.status.idle":"2023-11-20T16:59:15.725492Z","shell.execute_reply.started":"2023-11-20T16:59:15.652415Z","shell.execute_reply":"2023-11-20T16:59:15.723584Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m data_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomVerticalFlip(),\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transform\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Split into training and testing datasets\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_dataset))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/data'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/data'","output_type":"error"}]},{"cell_type":"code","source":"# Define the CNN model\nclass Net(nn.Module):\n    def __init__(self, num_classes=5):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.drop = nn.Dropout2d(p=0.2)\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.pool(self.conv1(x)))\n        x = F.relu(self.pool(self.conv2(x)))\n        x = F.dropout(self.drop(x), training=self.training)\n        x = x.view(-1, 32 * 32 * 24)\n        x = self.fc(x)\n        return torch.log_softmax(x, dim=1)\n\n# Instantiate the model\nmodel = Net(num_classes=len(classes))\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set up the optimizer and loss criterion\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nloss_criteria = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.726288Z","iopub.status.idle":"2023-11-20T16:59:15.726682Z","shell.execute_reply.started":"2023-11-20T16:59:15.726502Z","shell.execute_reply":"2023-11-20T16:59:15.726521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING FUNCTION","metadata":{}},{"cell_type":"code","source":"# Training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_criteria(output, target)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    avg_loss = train_loss / len(train_loader)\n    print('Epoch {}: Train set: Average loss: {:.6f}'.format(epoch, avg_loss))\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.728957Z","iopub.status.idle":"2023-11-20T16:59:15.730829Z","shell.execute_reply.started":"2023-11-20T16:59:15.730493Z","shell.execute_reply":"2023-11-20T16:59:15.730528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING FUNCTION","metadata":{}},{"cell_type":"code","source":"# Testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += loss_criteria(output, target).item()\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target == predicted).item()\n    avg_loss = test_loss / len(test_loader)\n    accuracy = correct / len(test_loader.dataset)\n    print('Test set: Average loss: {:.6f}, Accuracy: {:.2f}%'.format(avg_loss, accuracy * 100))\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.732564Z","iopub.status.idle":"2023-11-20T16:59:15.732948Z","shell.execute_reply.started":"2023-11-20T16:59:15.732773Z","shell.execute_reply":"2023-11-20T16:59:15.732790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING LOOP","metadata":{}},{"cell_type":"code","source":"# Training loop\nepochs = 10\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\nfor epoch in range(1, epochs + 1):\n    train_loss = train(model, device, train_loader, optimizer, epoch)\n    test_loss = test(model, device, test_loader)\n    epoch_nums.append(epoch)\n    training_loss.append(train_loss)\n    validation_loss.append(test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.734537Z","iopub.status.idle":"2023-11-20T16:59:15.735638Z","shell.execute_reply.started":"2023-11-20T16:59:15.735353Z","shell.execute_reply":"2023-11-20T16:59:15.735380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PLOTTING LOSS HISTORY","metadata":{}},{"cell_type":"code","source":"# Plot loss history\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(epoch_nums, training_loss, label='Training Loss')\nplt.plot(epoch_nums, validation_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.736849Z","iopub.status.idle":"2023-11-20T16:59:15.737759Z","shell.execute_reply.started":"2023-11-20T16:59:15.737474Z","shell.execute_reply":"2023-11-20T16:59:15.737501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONFUSION MATRIX","metadata":{}},{"cell_type":"code","source":"# Confusion matrix\ntruelabels = []\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        truelabels.extend(target.cpu().numpy())\n        predictions.extend(output.argmax(dim=1).cpu().numpy())\n        \n# Plot confusion matrix\ncm = confusion_matrix(truelabels, predictions, labels=range(len(classes)))\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.739137Z","iopub.status.idle":"2023-11-20T16:59:15.740299Z","shell.execute_reply.started":"2023-11-20T16:59:15.739987Z","shell.execute_reply":"2023-11-20T16:59:15.740016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Images","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Specify the main folder containing the subdirectories\nmain_folder = '/kaggle/working/Mango'\n\n# Specify the subdirectories\nsubdirectories = ['Alternaria', 'Anthracnose', 'Black Mould Rot', 'Stem end Rot', 'Healthy']\n\n# Display three sample images from each subdirectory\nplt.figure(figsize=(15, 15))\n\nfor i, subdir in enumerate(subdirectories):\n    subdir_path = os.path.join(main_folder, subdir)\n    image_files = [f for f in os.listdir(subdir_path) if f.endswith(('.jpg', '.jpeg', '.png'))][:3]\n    \n    for j, image_file in enumerate(image_files):\n        img_path = os.path.join(subdir_path, image_file)\n        img = mpimg.imread(img_path)\n\n        # Calculate the position in the subplot grid\n        position = i * 3 + j + 1\n\n        plt.subplot(5, 3, position)\n        plt.imshow(img)\n        plt.title(f\"{subdir}\\n{image_file}\")\n        plt.axis('off')\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:15.741557Z","iopub.status.idle":"2023-11-20T16:59:15.742081Z","shell.execute_reply.started":"2023-11-20T16:59:15.741823Z","shell.execute_reply":"2023-11-20T16:59:15.741849Z"},"trusted":true},"execution_count":null,"outputs":[]}]}